import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn import preprocessing
from sklearn.decomposition import PCA
import tensorflow as tf
from tensorflow import keras
import scipy as sp
import scipy.linalg as la
import sklearn as sk
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import roc_curve, auc

#Δημιουργία DataFrame με τα δεδομένα μας
data = pd.read_csv('winequality-white.csv', sep=";",header = 'infer')

#Επιλογή threshold
threshold = data.quality.median()
smaller = len(data.quality.loc[data.quality < threshold])
greater = len(data.quality.loc[data.quality > threshold])

print(threshold,smaller,greater)

#Δημιουργία Κλάσεων: 0 - Βαθμολογία<6, 1 - Βαθμολογία>=6
data['binQuality'] = [int(i>=threshold) for i in list(data.quality)]

#Δημιουργία Training και Test Set
trSet = (data.loc[data.binQuality == 0]).sample(frac=0.75)
data.drop(trSet.index,inplace=True)
temp = (data.loc[data.binQuality == 1]).sample(n=len(trSet))
trSet = pd.concat([trSet,temp])
testSet = data.loc[data.binQuality == 0]
temp = (data.loc[data.binQuality == 1]).sample(n=len(testSet))
testSet = pd.concat([testSet,temp])

#Επαναπροσδιορισμός Indexes για ευκολία στη χρήση
trSet = trSet.set_index(np.arange(len(trSet)))
testSet = testSet.set_index(np.arange(len(testSet)))

#Προπαρασκευή Δεδομένων
trSet.loc[trSet.binQuality==1].iloc[:,:11].plot(kind='hist',bins = 40, subplots = True,layout = (3,4), figsize = (30,20), sharex = True,title = "Excellent Quality")
trSet.loc[trSet.binQuality==0].iloc[:,:11].plot(kind='hist',bins = 40, subplots = True,layout = (3,4), figsize = (30,20), sharex = True,title = "Poor Quality")

for j in list(trSet)[:-2]:
    #Αφαίρεση ακραίων τιμών
    m_e=(trSet.loc[trSet.binQuality == 1])[j].mean()
    m_p=(trSet.loc[trSet.binQuality == 0])[j].mean()
    std_e=(trSet.loc[trSet.binQuality == 1])[j].std()
    std_p=(trSet.loc[trSet.binQuality == 0])[j].std()
    trSet.drop((trSet.loc[trSet.binQuality == 1]).loc[abs((trSet[j]-m_e))>(5*(std_e))].index,inplace = True)
    trSet.drop((trSet.loc[trSet.binQuality == 0]).loc[abs((trSet[j]-m_p))>(5*(std_p))].index,inplace = True)
    
nindex = np.arange(len(trSet.index))
trSet = trSet.set_index(nindex)

#Απεικόνιση συνόλου εκπαίδευσης
fig, axis = plt.subplots(6,2,figsize=(40,70))
x,y=(0,0)
for i in list(trSet)[:-2]:
    sns.kdeplot(trSet.loc[trSet.binQuality == 1][i],shade=True,ax=axis[x,y])
    ax = sns.kdeplot(trSet.loc[trSet.binQuality == 0][i],shade=True,ax=axis[x,y])
    ax.set_title('Κατανομή χαρακτηριστικού ' +i);
    y+=1
    if(y%2==0):
        y=0
        x+=1
        
#Correlation Matrices
cor_mat = trSet.loc[trSet.binQuality == 1].corr()
fig, ax=plt.subplots(figsize=(10,10))
ax = sns.heatmap(cor_mat,annot=True,ax=ax)
ax.set(title='Excellent Quality')
plt.show()

cor_mat = trSet.loc[trSet.binQuality == 0].corr()
fig, ax=plt.subplots(figsize=(10,10))
ax = sns.heatmap(cor_mat,annot=True,ax=ax)
ax.set(title='Poor Quality')
plt.show()

#Κανονικοποίηση
scaler = preprocessing.StandardScaler().fit(trSet.iloc[:,:-2])
train_scaled = scaler.transform(trSet.iloc[:,:-2])
test_scaled = scaler.transform(testSet.iloc[:,:-2])

#Ανάλυση PCA
pca = PCA().fit(train_scaled)
print(pca.explained_variance_)

plt.plot(np.arange(1,12),np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance');
plt.show()

pca = PCA(n_components=9)

trainPCA = pca.fit_transform(train_scaled)
testPCA = pca.transform(test_scaled)

#Κατανομή ιδιοτιμών πίνακα διασποράς
covMatrix = pd.DataFrame(train_scaled).cov()

res = la.eig(covMatrix)
eigenValues = [i.real for i in res[0]]
eigenVectors = [i.real for i in res[1]]
plt.bar([list(trSet.iloc[:,:-2])[eigenValues.index(i)] for i in sorted(eigenValues, reverse = True) ],sorted(eigenValues, reverse = True))
plt.xticks(rotation=90)
plt.xlabel("Eigenvalue Index")
plt.ylabel("Eigenvalue")
plt.show()

#Βελτιστοποίηση παραμέτρων
score = []
for i in range(1,10):
    r = 0.1*i
    clf_LDA = LinearDiscriminantAnalysis(solver="lsqr",shrinkage=r)
    clf_LDA.fit(trainPCA,trSet.binQuality)
    score.append(clf_LDA.score(trainPCA,trSet.binQuality))
    
r = 0.1*score.index(max(score))
print(score)

#Γραμμικός Ταξινομητής Ελαχίστων Τετραγώνων
clf_LDA = LinearDiscriminantAnalysis(solver="lsqr",shrinkage=r)
clf_LDA.fit(trSet.iloc[:,:-2],trSet.binQuality)

W_LDA = clf_LDA.coef_[0]
W0_LDA = clf_LDA.intercept_

print("===== LDA WEIGHTS =======")
print("W = ",W_LDA)
print("W0 = ",W0_LDA)
print("Normalized W = ",W_LDA/W_LDA[0])
print("Normalized W0 = ",W0_LDA/W_LDA[0])

#Neural Network
temp = pd.DataFrame(trainPCA)
temp['binQuality'] = trSet.binQuality
trSet_NN = temp.copy()
validSet = temp.sample(frac=0.20)
trSet_NN.drop(validSet.index,inplace=True)
tr = pd.DataFrame(trSet_NN.binQuality)
val = pd.DataFrame(validSet.binQuality)

model = keras.Sequential([
    keras.layers.Flatten(input_shape=(9,)),    # input layer (1)
    keras.layers.Dense(3, activation='relu'),   # hidden layer (2)
    keras.layers.Dense(1, activation='sigmoid') # output layer (3)
])
model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])

trloss = []
valloss = []

for i in np.arange(0,100,2):

    model.fit(pd.DataFrame(trSet_NN.iloc[:,:-1]), tr, epochs=i)

    val_loss, val_acc = model.evaluate(pd.DataFrame(validSet.iloc[:,:-1]), val, verbose=0)
    tr_loss, tr_acc = model.evaluate(pd.DataFrame(trSet_NN.iloc[:,:-1]), tr, verbose=0)
    trloss.append(tr_loss)
    valloss.append(val_loss)
    
fig, ax = plt.subplots()
ax.plot(np.arange(0,100,2), valloss, label = 'validation')
ax.plot(np.arange(0,100,2), trloss, label = 'training')
leg = ax.legend()
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.show()

model.fit(pd.DataFrame(trSet_NN.iloc[:,:-1]), tr, epochs=55)

#Boosted Decision Tree
tree = DecisionTreeClassifier(max_depth = 3)
clf_bdtree = AdaBoostClassifier(n_estimators = 200, learning_rate = 0.5, base_estimator = tree)
clf_bdtree = clf_bdtree.fit(trainPCA, trSet['binQuality'])

#ROC Curves
#Find false positive & true positive rates.
false_pos_f, true_pos_f, thres_f = roc_curve(testSet.binQuality, clf_LDA.predict_proba(testPCA)[:,1])
false_pos_nn, true_pos_nn, thres_nn = roc_curve(testSet.binQuality, model.predict(testPCA))
false_pos_tree, true_pos_tree, thres_tree = roc_curve(testSet.binQuality, clf_bdtree.predict_proba(testPCA)[:,1])

fig, axs = plt.subplots()
axs.plot(false_pos_f, true_pos_f, label = "LSM")
axs.plot(false_pos_nn, true_pos_nn, label = "Neural Network")
axs.plot(false_pos_tree, true_pos_tree, label = "Boosted Decision Tree")
axs.legend()
plt.xlabel("False Positive")
plt.ylabel("True Positive")
plt.show()

print("roc_auc_LSM = ", auc(false_pos_f, true_pos_f))
print("roc_auc_NN = ", auc(false_pos_nn, true_pos_nn))
print("roc_auc_BDT = ", auc(false_pos_tree, true_pos_tree))

# LSM
clf_1 = LinearDiscriminantAnalysis(solver="lsqr",shrinkage=r)
clf_1.fit(trSet.iloc[:,:-2],trSet.binQuality)

W_LDA = clf_1.coef_[0]
W0_LDA = clf_1.intercept_

print("===== LDA WEIGHTS =======")
print("W = ",W_LDA)
print("W0 = ",W0_LDA)
print("Normalized W = ",W_LDA/W_LDA[0])
print("Normalized W0 = ",W0_LDA/W_LDA[0])


# Boosted Decision Tree
tree = DecisionTreeClassifier(max_depth = 3)
clf_2 = AdaBoostClassifier(n_estimators = 200, learning_rate = 0.5, base_estimator = tree)
clf_2 = clf_2.fit(trSet.iloc[:,:-2], trSet['binQuality'])


#Neural Network:
clf_3 = MLPClassifier(
hidden_layer_sizes=(8),
activation="relu",
verbose=False,
max_iter=350
)

clf_3.fit(trSet.iloc[:,:-2], trSet.binQuality)

# LSM
scaler = preprocessing.StandardScaler().fit(trSet.iloc[:,:-2])
train_scaled = pd.DataFrame(scaler.transform(trSet.iloc[:,:-2]))
test_scaled = pd.DataFrame(scaler.transform(testSet.iloc[:,:-2]))

clf_1 = LinearDiscriminantAnalysis(solver="lsqr",shrinkage=r)
clf_1.fit(train_scaled,trSet.binQuality)
W_LDA = clf_1.coef_[0]
W0_LDA = clf_1.intercept_

#LDA:
coefs = pd.DataFrame(
   clf_1.coef_.transpose(), columns = ['Coefficients'], index = np.array(list(testSet.iloc[:,:-2]))
       )

coefs.plot(kind='barh', figsize=(9, 7))
plt.title('Coefficients')
plt.axvline(x=0, color='.4')
plt.subplots_adjust(left=.3)

#Boosted Decision Tree:

importances = clf_2.feature_importances_

indices = np.argsort(importances)

fig, ax = plt.subplots()
ax.barh(range(len(importances)), importances[indices])
ax.set_yticks(range(len(importances)))
_ = ax.set_yticklabels(np.array(list(trSet.iloc[:,:-2]))[indices])
plt.show()

#Neural Network:
from sklearn.metrics import accuracy_score
from copy import deepcopy

y_pred = clf_3.predict(testSet.iloc[:,:-2])
y_test = testSet.binQuality

def get_feature_importance(j, n):
    s = accuracy_score(testSet.binQuality, y_pred) # baseline score
    total = 0.0
    for i in range(n):
        perm = np.random.permutation(range(testSet.shape[0]))
        testSet_ = deepcopy(testSet.iloc[:,:-2].to_numpy())
        for i in range(len(testSet_[:,j])):
            testSet_[i,j] = testSet_[perm[i], j]
        y_pred_ = clf_3.predict(testSet_)
        s_ij = accuracy_score(y_test, y_pred_)
        total += s_ij
    return s - (total / n)

# Plot
f = []
for j in range(testSet.iloc[:,:-2].shape[1]):
    f_j = get_feature_importance(j, 100)
    f.append(f_j)

plt.figure(figsize=(10, 5))
plt.bar(range(testSet.iloc[:,:-2].shape[1]), f, color="r", alpha=0.7)
plt.xticks(ticks = range(testSet.iloc[:,:-2].shape[1]), labels = list(testSet.iloc[:,:-2]), rotation = 90)
plt.xlabel("Feature")
plt.ylabel("Importance")
plt.title("Feature importances (Wine Quality)")
plt.show()
